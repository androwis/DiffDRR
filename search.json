[
  {
    "objectID": "tutorials/metamorphasis.html",
    "href": "tutorials/metamorphasis.html",
    "title": "Converting to DeepDRR",
    "section": "",
    "text": "from inspect import getfile\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\n\nimport deepdrr\nfrom deepdrr import geo, Volume, MobileCArm, Projector\nfrom deepdrr.load_dicom import load_dicom\nfrom diffdrr.drr import DRR\nfrom diffdrr.metrics import XCorr2\n\n\nSDR = 400\nP = 4.0\n\n\n# Load a DICOM and extract voxel information\nexample_ct_path = str(Path(getfile(DRR)).parent / \"data/cxr\") + \"/\"\nvolume, materials, spacing = load_dicom(example_ct_path)\n\n# Make volume conventions same as DiffDR\npreprocess = lambda x: np.rot90(x, -1)[:, ::-1]\nvolume = preprocess(volume)\nfor key, value in materials.items():\n    materials[key] = preprocess(value)\n\n# Use the center of the volume as the \"world\" coordinates. The origin is the (0, 0, 0) index of the volume in the world frame.\nvol_center = (np.array(volume.shape) - 1) / 2 * spacing\norigin = geo.point(-vol_center[0], -vol_center[1], -vol_center[2])\n\n# Create the volume object with segmentation\npatient = Volume.from_parameters(\n    data=volume,\n    materials=materials,\n    origin=origin,\n    spacing=spacing,\n    anatomical_coordinate_system=\"LPS\",\n)\npatient.orient_patient(head_first=True, supine=True)\n\nUsing downloaded and verified file: /home/vivekg/datasets/DeepDRR_DATA/model_segmentation.pth.tar\n\n\n\n# defines the C-Arm device, which is a convenience class for positioning the Camera.\n# isocenter=volume.center_in_world\ncarm = MobileCArm(\n    isocenter=patient.center_in_world,\n    rotate_camera_left=False,\n    source_to_detector_distance=SDR*2,\n    source_to_isocenter_vertical_distance=SDR,\n    pixel_size=P,\n    sensor_height=256,\n    sensor_width=256,\n    min_alpha=-720, max_alpha=720, min_beta=-720, max_beta=720\n)\n\ndef test_phantom_deepdrr(theta, phi, gamma):\n    with Projector(\n        volume=patient,\n        carm=carm,\n    ) as projector:\n        carm.move_to(\n            alpha=np.rad2deg(theta),\n            beta=np.rad2deg(np.pi/2-phi),\n            gamma=np.rad2deg(-gamma),\n            degrees=True,\n        )\n        img = projector()  # The first run doesn't use updated parameters, for some reason?\n        img = projector()[:, ::-1].copy()\n    return img\n\n\ndef test_phantom_diffdrr(theta, phi, gamma, sdr=SDR, p=P):\n    bx, by, bz = (torch.tensor(volume.shape) - 1) * torch.tensor(spacing) / 2\n    drr = DRR(volume, spacing, sdr=SDR, height=256, delx=P, convention=\"deepdrr\")\n    drr.move_carm(\n        rotations=torch.tensor([[theta, phi, gamma]]),\n        translations=torch.tensor([[bx, by, bz]]),\n    )\n    img = drr().detach()\n    img = img / img.max()\n    return img\n\n\nfor idx in range(5):\n    theta = np.random.uniform(-torch.pi, torch.pi)\n    phi = np.random.uniform(-torch.pi, torch.pi)\n    gamma = np.random.uniform(-torch.pi, torch.pi)\n    diff = test_phantom_diffdrr(theta, phi, gamma).squeeze().numpy()\n    deep = test_phantom_deepdrr(theta, phi, gamma)\n    metric = XCorr2()(torch.tensor(diff[np.newaxis, np.newaxis, ...]),\n                      torch.tensor(deep[np.newaxis, np.newaxis, ...])).item()\n\n    plt.figure(figsize=(12, 3))\n    plt.subplot(131)\n    plt.title(\"DiffDRR\")\n    plt.imshow(diff, cmap=\"gray\")\n    plt.colorbar()\n    plt.subplot(132)\n    plt.title(\"DeepDRR\")\n    plt.imshow(deep, cmap=\"gray\")\n    plt.colorbar()\n    plt.subplot(133)\n    plt.title(f\"XCorr={metric:.5g}\")\n    plt.imshow(deep-diff, cmap=\"gray\")\n    plt.colorbar()\n    plt.tight_layout()\n    plt.show()"
  },
  {
    "objectID": "tutorials/metrics.html",
    "href": "tutorials/metrics.html",
    "title": "Registration loss landscapes",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\nimport torch\nfrom tqdm import tqdm\n\nfrom diffdrr.drr import DRR\nfrom diffdrr.data import load_example_ct\nfrom diffdrr.metrics import XCorr2\nfrom diffdrr.visualization import plot_drr"
  },
  {
    "objectID": "tutorials/metrics.html#utility-functions-for-the-simulation",
    "href": "tutorials/metrics.html#utility-functions-for-the-simulation",
    "title": "Registration loss landscapes",
    "section": "Utility functions for the simulation",
    "text": "Utility functions for the simulation\n\nGenerate ground truth DRR\nFunction for generating estimated DRRs\nFunctions for scoring (Negative NCC and L2-norm)\n\n\n# Read in the volume\nvolume, spacing = load_example_ct()\ndrr = DRR(volume, spacing, sdr=300.0, height=200, delx=4.0).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Get parameters for the detector\ntheta, phi, gamma = torch.pi, 0.0, torch.pi / 2\nbx, by, bz = torch.tensor(volume.shape) * torch.tensor(spacing) / 2\nrotations = torch.tensor([[theta, phi, gamma]])\ntranslations = torch.tensor([[bx, by, bz]])\n\n# Make the DRR\ndrr.move_carm(rotations, translations)\ntarget_drr = drr().detach()\nax = plot_drr(target_drr)\nplt.show()\n\n\n\n\nInteresting implementation detail here: the output of the forward pass, drr(), has to be .detached. Otherwise, the individual computational graph used to generate each DRR will be stored in GPU memory and cause a CUDA error! Note that when normally training a network, each backward pass automatically clears the graph, so these memory issues are not a concern.\nSee this thread for more details.\n\n# Scoring functions\nxcorr2 = XCorr2(zero_mean_normalized=True)\n\ndef get_normxcorr2(theta, phi, gamma, bx, by, bz):\n    drr.move_carm(\n        torch.tensor([[theta, phi, gamma]]),\n        torch.tensor([[bx, by, bz]]),\n    )\n    moving_drr = drr().detach()  # Explicitly detach the generated DRR\n    return xcorr2(target_drr, moving_drr)"
  },
  {
    "objectID": "tutorials/metrics.html#negative-normalized-xcorr",
    "href": "tutorials/metrics.html#negative-normalized-xcorr",
    "title": "Registration loss landscapes",
    "section": "Negative Normalized XCorr",
    "text": "Negative Normalized XCorr\n\n# NCC for the XYZs\nxs = torch.arange(-15., 16.)\nys = torch.arange(-15., 16.)\nzs = torch.arange(-15., 16.)\n\n# Get coordinate-wise correlations\nxy_corrs = []\nfor x in tqdm(xs):\n    for y in ys:\n        xcorr = get_normxcorr2(theta, phi, gamma, bx+x, by+y, bz)\n        xy_corrs.append(-xcorr)\nXY = torch.tensor(xy_corrs).reshape(len(xs), len(ys))\n        \nxz_corrs = []\nfor x in tqdm(xs):\n    for z in zs:\n        xcorr = get_normxcorr2(theta, phi, gamma, bx+x, by, bz+z)\n        xz_corrs.append(-xcorr)\nXZ = torch.tensor(xz_corrs).reshape(len(xs), len(zs))\n        \nyz_corrs = []\nfor y in tqdm(ys):\n    for z in zs:\n        xcorr = get_normxcorr2(theta, phi, gamma, bx, by+y, bz+z)\n        yz_corrs.append(-xcorr)\nYZ = torch.tensor(yz_corrs).reshape(len(ys), len(zs))\n\n100%|██████████████████████████████████████████████████████████████████████████████| 31/31 [00:34&lt;00:00,  1.10s/it]\n100%|██████████████████████████████████████████████████████████████████████████████| 31/31 [00:34&lt;00:00,  1.10s/it]\n100%|██████████████████████████████████████████████████████████████████████████████| 31/31 [00:34&lt;00:00,  1.11s/it]\n\n\n\n# NCC for the angles\nt_angles = torch.arange(-torch.pi/4, torch.pi/4, step=.05)\np_angles = torch.arange(-torch.pi/4, torch.pi/4, step=.05)\ng_angles = torch.arange(-torch.pi/8, torch.pi/8, step=.05)\n\n# Get coordinate-wise correlations\ntp_corrs = []\nfor t in tqdm(t_angles):\n    for p in p_angles:\n        xcorr = get_normxcorr2(theta+t, phi+p, gamma, bx, by, bz)\n        tp_corrs.append(-xcorr)\nTP = torch.tensor(tp_corrs).reshape(len(t_angles), len(p_angles))\n        \ntg_corrs = []\nfor t in tqdm(t_angles):\n    for g in g_angles:\n        xcorr = get_normxcorr2(theta+t, phi, gamma+g, bx, by, bz)\n        tg_corrs.append(-xcorr)\nTG = torch.tensor(tg_corrs).reshape(len(t_angles), len(g_angles))\n        \npg_corrs = []\nfor p in tqdm(p_angles):\n    for g in g_angles:\n        xcorr = get_normxcorr2(theta, phi+p, gamma+g, bx, by, bz)\n        pg_corrs.append(-xcorr)\nPG = torch.tensor(pg_corrs).reshape(len(p_angles), len(g_angles))\n\n100%|██████████████████████████████████████████████████████████████████████████████| 32/32 [00:41&lt;00:00,  1.30s/it]\n100%|██████████████████████████████████████████████████████████████████████████████| 32/32 [00:20&lt;00:00,  1.57it/s]\n100%|██████████████████████████████████████████████████████████████████████████████| 32/32 [00:18&lt;00:00,  1.69it/s]\n\n\n\n# Make the plots\n\n# XYZ\nxyx, xyy = torch.meshgrid(xs, ys)\nxzx, xzz = torch.meshgrid(xs, zs)\nyzy, yzz = torch.meshgrid(ys, zs)\n\nfig = plt.figure(figsize=3*plt.figaspect(1.2/1), dpi=300)\n\nax = fig.add_subplot(1, 3, 1, projection='3d')\nax.contourf(xyx, xyy, XY, zdir=\"z\", offset=-1, cmap=plt.get_cmap('rainbow'), alpha=0.5)\nax.plot_surface(xyx, xyy, XY, rstride=1, cstride=1, cmap=plt.get_cmap('rainbow'))\nax.set_xlabel('ΔX (mm)')\nax.set_ylabel('ΔY (mm)')\nax.set_zlim3d(-1., -0.825)\n\nax = fig.add_subplot(1, 3, 2, projection='3d')\nax.contourf(xzx, xzz, XZ, zdir=\"z\", offset=-1, cmap=plt.get_cmap('rainbow'), alpha=0.5)\nax.plot_surface(xzx, xzz, XZ, rstride=1, cstride=1, cmap=plt.get_cmap('rainbow'))\nax.set_xlabel('ΔX (mm)')\nax.set_ylabel('ΔZ (mm)')\nax.set_zlim3d(-1., -0.825)\n\nax = fig.add_subplot(1, 3, 3, projection='3d')\nax.contourf(yzy, yzz, YZ, zdir=\"z\", offset=-1, cmap=plt.get_cmap('rainbow'), alpha=0.5)\nax.plot_surface(yzy, yzz, YZ, rstride=1, cstride=1, cmap=plt.get_cmap('rainbow'))\nax.set_xlabel('ΔY (mm)')\nax.set_ylabel('ΔZ (mm)')\nax.set_zlim3d(-1., -0.825)\n\n\n# Angles\nxyx, xyy = torch.meshgrid(t_angles, p_angles)\nxzx, xzz = torch.meshgrid(t_angles, g_angles)\nyzy, yzz = torch.meshgrid(p_angles, g_angles)\n\nax = fig.add_subplot(2, 3, 1, projection='3d')\nax.contourf(xyx, xyy, TP, zdir=\"z\", offset=-1, cmap=plt.get_cmap('rainbow'), alpha=0.5)\nax.plot_surface(xyx, xyy, TP, rstride=1, cstride=1, cmap=plt.get_cmap('rainbow'))\nax.set_xlabel('Δθ (radians)')\nax.set_ylabel('Δφ (radians)')\nax.set_zlim3d(-1., -0.4)\n\nax = fig.add_subplot(2, 3, 2, projection='3d')\nax.contourf(xzx, xzz, TG, zdir=\"z\", offset=-1, cmap=plt.get_cmap('rainbow'), alpha=0.5)\nax.plot_surface(xzx, xzz, TG, rstride=1, cstride=1, cmap=plt.get_cmap('rainbow'))\nax.set_xlabel('Δθ (radians)')\nax.set_ylabel('Δγ (radians)')\nax.set_zlim3d(-1., -0.4)\n\nax = fig.add_subplot(2, 3, 3, projection='3d')\nax.contourf(yzy, yzz, PG, zdir=\"z\", offset=-1, cmap=plt.get_cmap('rainbow'), alpha=0.5)\nax.plot_surface(yzy, yzz, PG, rstride=1, cstride=1, cmap=plt.get_cmap('rainbow'))\nax.set_xlabel('Δφ (radians)')\nax.set_ylabel('Δγ (radians)')\nax.set_zlim3d(-1., -0.4)\n\nplt.show()\n\n/data/vision/polina/users/vivekg/utils/mambaforge/envs/diffdrr/lib/python3.11/site-packages/torch/functional.py:505: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3490.)\n  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]"
  },
  {
    "objectID": "tutorials/timing.html",
    "href": "tutorials/timing.html",
    "title": "Timing versus DRR size",
    "section": "",
    "text": "import numpy as np\nimport torch\n\nfrom diffdrr.drr import DRR\nfrom diffdrr.data import load_example_ct\nfrom diffdrr.visualization import plot_drr\n\n\n# Read in the volume\nvolume, spacing = load_example_ct()\n\n# Get parameters for the detector\nbx, by, bz = np.array(volume.shape) * np.array(spacing) / 2\ntranslations = torch.tensor([[bx, by, bz]])\nrotations = torch.tensor([[np.pi, 0, np.pi / 2]])\n\n\nheight = 100\n\ndrr = DRR(volume, spacing, sdr=300.0, height=height, delx=4.0).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndrr.move_carm(rotations, translations)\n\ndel drr\n\n10.4 ms ± 282 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\n\n\nheight = 200\n\ndrr = DRR(volume, spacing, sdr=300.0, height=height, delx=4.0).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndrr.move_carm(rotations, translations)\n\ndel drr\n\n34.9 ms ± 8.64 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\n\nheight = 300\n\ndrr = DRR(volume, spacing, sdr=300.0, height=height, delx=4.0).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndrr.move_carm(rotations, translations)\n\ndel drr\n\n74.4 ms ± 89.3 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\n\nheight = 400\n\ndrr = DRR(volume, spacing, sdr=300.0, height=height, delx=4.0).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndrr.move_carm(rotations, translations)\n\ndel drr\n\n126 ms ± 237 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\n\nheight = 500\n\ndrr = DRR(volume, spacing, sdr=300.0, height=height, delx=4.0).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndrr.move_carm(rotations, translations)\n\ndel drr\n\nOutOfMemoryError: CUDA out of memory. Tried to allocate 2.10 GiB (GPU 0; 10.75 GiB total capacity; 6.08 GiB already allocated; 1.73 GiB free; 8.17 GiB reserved in total by PyTorch) If reserved memory is &gt;&gt; allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
  },
  {
    "objectID": "tutorials/optimizers.html",
    "href": "tutorials/optimizers.html",
    "title": "2D-to-3D registration",
    "section": "",
    "text": "from pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom tqdm import tqdm\n\nfrom diffdrr.drr import DRR\nfrom diffdrr.data import load_example_ct\nfrom diffdrr.metrics import XCorr2\nfrom diffdrr.visualization import plot_drr\n\nnp.random.seed(39)\n\n\ndef converged(df):\n    return df[\"loss\"].iloc[-1] &lt;= -0.999\n\n\ndefaults = (\"cuda\" if torch.cuda.is_available() else \"cpu\", torch.float32)\nprint(defaults)\n\n('cuda', torch.float32)\n\n\n\n# Make the ground truth X-ray\nSDR = 300.0\nHEIGHT = 100\nDELX = 8.0\n\nvolume, spacing = load_example_ct()\nbx, by, bz = np.array(volume.shape) * np.array(spacing) / 2\ntrue_params = {\n    \"sdr\": SDR,\n    \"theta\": torch.pi,\n    \"phi\": 0,\n    \"gamma\": torch.pi / 2,\n    \"bx\": bx,\n    \"by\": by,\n    \"bz\": bz,\n}\n\ndrr = DRR(volume, spacing, sdr=SDR, height=HEIGHT, delx=DELX).to(*defaults)\nrotations = torch.tensor([[true_params[\"theta\"], true_params[\"phi\"], true_params[\"gamma\"]]])\ntranslations = torch.tensor([[true_params[\"bx\"], true_params[\"by\"], true_params[\"bz\"]]])\n\ndrr.move_carm(rotations, translations)\nground_truth = drr()\n\nplot_drr(ground_truth)\nplt.show()\n\n\n\n\n\n# Make a random DRR\ndef get_initial_parameters(true_params):\n    theta = true_params[\"theta\"] + np.random.uniform(-np.pi / 4, np.pi / 4)\n    phi = true_params[\"phi\"] + np.random.uniform(-np.pi / 3, np.pi / 3)\n    gamma = true_params[\"gamma\"] + np.random.uniform(-np.pi / 3, np.pi / 3)\n    bx = true_params[\"bx\"] + np.random.uniform(-30.0, 31.0)\n    by = true_params[\"by\"] + np.random.uniform(-30.0, 31.0)\n    bz = true_params[\"bz\"] + np.random.uniform(-30.0, 31.0)\n    return torch.tensor([[theta, phi, gamma]]), torch.tensor([[bx, by, bz]])\n\n\nrotations, translations = get_initial_parameters(true_params)\ndrr = DRR(volume, spacing, sdr=SDR, height=HEIGHT, delx=DELX)\ndrr.move_carm(rotations, translations)\nest = drr().detach()\n\nplot_drr(est)\nplt.show()\n\n\n\n\n\ndef optimize(\n    drr,\n    ground_truth,\n    lr_rotations=5.3e-2,\n    lr_translations=7.5e1,\n    momentum=0,\n    dampening=0,\n    n_itrs=250\n):\n    criterion = XCorr2(zero_mean_normalized=True)\n    optimizer = torch.optim.SGD(\n        [\n            {\"params\": [drr.rotations], \"lr\": lr_rotations},\n            {\"params\": [drr.translations], \"lr\": lr_translations},\n        ],\n        momentum=momentum,\n        dampening=dampening,\n    )\n    \n    params = []\n    for itr in tqdm(range(n_itrs)):\n        # Save the current set of parameters\n        theta, phi, gamma = drr.rotations.squeeze().tolist()\n        bx, by, bz = drr.translations.squeeze().tolist()\n        params.append([i for i in [theta, phi, gamma, bx, by, bz]])\n\n        # Run the optimization loop\n        optimizer.zero_grad()\n        estimate = drr()\n        loss = -criterion(ground_truth, estimate)\n        loss.backward(retain_graph=True)\n        optimizer.step()\n        \n        if loss &lt; -0.999:\n            tqdm.write(f\"Converged in {itr} iterations\")\n            break\n        \n    return pd.DataFrame(params, columns=[\"theta\", \"phi\", \"gamma\", \"bx\", \"by\", \"bz\"])\n\n\n# Base SGD\ndrr = DRR(volume, spacing, sdr=SDR, height=HEIGHT, delx=DELX).to(*defaults)\ndrr.move_carm(rotations, translations)\nparams_base = optimize(drr, ground_truth)\ndel drr\n\n# SGD + momentum\ndrr = DRR(volume, spacing, sdr=SDR, height=HEIGHT, delx=DELX).to(*defaults)\ndrr.move_carm(rotations, translations)\nparams_momentum = optimize(drr, ground_truth, momentum=0.9)\ndel drr\n\n# SGD + momentum + dampening\ndrr = DRR(volume, spacing, sdr=SDR, height=HEIGHT, delx=DELX).to(*defaults)\ndrr.move_carm(rotations, translations)\nparams_momentum_dampen = optimize(drr, ground_truth, momentum=0.9, dampening=0.1)\ndel drr\n\n 55%|█████████████████████████████████████████▉                                  | 138/250 [00:03&lt;00:03, 35.27it/s]\n 25%|███████████████████                                                          | 62/250 [00:01&lt;00:05, 35.07it/s]\n 23%|█████████████████▊                                                           | 58/250 [00:01&lt;00:05, 34.86it/s]\n\n\nConverged in 138 iterations\nConverged in 62 iterations\nConverged in 58 iterations\n\n\n\nfrom IPython.display import display, HTML\nfrom base64 import b64encode\n\nfrom diffdrr.visualization import animate\n\n\ndef animate_in_browser(df):\n    out = animate(\"&lt;bytes&gt;\", df, drr, ground_truth, verbose=True, extension=\".webp\", duration=30)\n    display(HTML(f\"\"\"&lt;img src='{\"data:img/gif;base64,\" + b64encode(out).decode()}'&gt;\"\"\"))\n\n\ndrr = DRR(volume, spacing, sdr=SDR, height=HEIGHT, delx=DELX).to(*defaults)\n\n\nanimate_in_browser(params_base)\n\nPrecomputing DRRs: 100%|█████████████████████████████████████████████████████████| 139/139 [00:25&lt;00:00,  5.53it/s]\n\n\n\n\n\n\nanimate_in_browser(params_momentum)\n\nPrecomputing DRRs: 100%|███████████████████████████████████████████████████████████| 63/63 [00:11&lt;00:00,  5.37it/s]\n\n\n\n\n\n\nanimate_in_browser(params_momentum_dampen)\n\nPrecomputing DRRs: 100%|███████████████████████████████████████████████████████████| 59/59 [00:10&lt;00:00,  5.55it/s]"
  },
  {
    "objectID": "tutorials/introduction.html",
    "href": "tutorials/introduction.html",
    "title": "How to use DiffDRR",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport torch\n\nfrom diffdrr.drr import DRR\nfrom diffdrr.data import load_example_ct\nfrom diffdrr.visualization import plot_drr"
  },
  {
    "objectID": "tutorials/introduction.html#drr-generation",
    "href": "tutorials/introduction.html#drr-generation",
    "title": "How to use DiffDRR",
    "section": "DRR Generation",
    "text": "DRR Generation\nDiffDRR is implemented as a custom PyTorch module.\nAll raytracing operations have been formulated in a vectorized function, enabling use of PyTorch’s GPU support and autograd. This also means that DRR generation is available as a layer in deep learning frameworks.\n\n# Read in the volume\nvolume, spacing = load_example_ct()\nbx, by, bz = torch.tensor(volume.shape) * torch.tensor(spacing) / 2\n\n# Rotations and translations determine the viewing angle\n# They must have the same batch_size as was passed to the DRR constructor\n# Rotations are (yaw pitch roll)\n# Translations are (bx by bz)\nrotations = torch.tensor([[torch.pi, 0.0, torch.pi / 2]])\ntranslations = torch.tensor([[bx, by, bz]])\n\n# Initialize the DRR module for generating synthetic X-rays\ndrr = DRR(\n    volume,        # The CT volume as a numpy array\n    spacing,       # Voxel dimensions of the CT\n    sdr=300.0,     # Source-to-detector radius (half of the source-to-detector distance)\n    height=200,    # Height of the DRR (if width is not seperately provided, the generated image is square)\n    delx=4.0,      # Pixel spacing (in mm)\n).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Make the DRR\ndrr.move_carm(rotations, translations)\nimg = drr().detach()\nax = plot_drr(img, ticks=False)\nplt.show()\n\n\n\n\nWe demonstrate the speed of DiffDRR by timing repeated DRR synthesis. Timing results are on a single NVIDIA RTX 2080 Ti GPU.\n\n\n\n34.9 ms ± 44.9 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\n\nSparse rendering\nYou can also render random sparse subsets of the pixels in a DRR. This is useful for speeding up registration tasks.\n\n# Make the DRR with 10% of the pixels\ndrr = DRR(\n    volume,\n    spacing,\n    sdr=300.0,\n    height=200,\n    delx=4.0,\n    p_subsample=0.1,  # Set the proportion of pixels that should be rendered\n    reshape=True,     # Map rendered pixels back to their location in true space, \n                      # Useful for plotting, but can be disabled if using MSE as a loss function\n).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Make the DRR\ndrr.move_carm(rotations, translations)\nimg = drr().detach()\nax = plot_drr(img, ticks=False)\nplt.show()\n\n\n\n\n\n\n\n7.74 ms ± 214 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)"
  },
  {
    "objectID": "tutorials/introduction.html#batched-drr-synthesis",
    "href": "tutorials/introduction.html#batched-drr-synthesis",
    "title": "How to use DiffDRR",
    "section": "Batched DRR synthesis",
    "text": "Batched DRR synthesis\n\n# Initialize the DRR module for generating synthetic X-rays\ndrr = DRR(\n    volume,        # The CT volume as a numpy array\n    spacing,       # Voxel dimensions of the CT\n    sdr=300.0,     # Source-to-detector radius (half of the source-to-detector distance)\n    height=200,    # Height of the DRR (if width is not seperately provided, the generated image is square)\n    delx=4.0,      # Pixel spacing (in mm)\n    batch_size=2,  # How many batches of parameters will be passed = number of DRRs generated each forward pass\n).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nrotations = torch.tensor([[torch.pi, 0.0, torch.pi / 2], [-torch.pi, 0.0, -torch.pi / 2]])\ntranslations = torch.tensor([[bx, by, bz], [bx, by, bz]])\ndrr.move_carm(rotations, translations)\nimg = drr().detach()\nplot_drr(img, ticks=False)\nplt.show()"
  },
  {
    "objectID": "tutorials/spherical.html",
    "href": "tutorials/spherical.html",
    "title": "Spherical coordinates",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport numpy as np\nimport torch\n\nfrom diffdrr.drr import DRR"
  },
  {
    "objectID": "tutorials/spherical.html#diffdrr-camera-matrix-conventions",
    "href": "tutorials/spherical.html#diffdrr-camera-matrix-conventions",
    "title": "Spherical coordinates",
    "section": "DiffDRR camera matrix conventions",
    "text": "DiffDRR camera matrix conventions\n\n\nCode\nsdr = 1.0\ntheta, gamma, phi = 0, 0, 0\nbx, by, bz = 10, -10, -40\nrotations = torch.Tensor([[theta, phi, gamma]])\ntranslations = torch.tensor([[bx, by, bz]])\n\ndrr = DRR(\n    volume=np.zeros([512, 512, 133]),\n    spacing=[1, 1, 1],\n    sdr=sdr,\n    height=5,\n    delx=0.75,\n    convention=\"diffdrr\",\n).to(\"cpu\")\n\ndrr.move_carm(rotations, translations)\n\n\n\nfig = plt.figure()\nfor i in range(8):\n    theta = (i / 8) * torch.pi\n    phi = (0 / 8) * torch.pi\n    gamma = (0 / 8) * torch.pi\n    rotations = torch.Tensor([[theta, phi, gamma]])\n    \n    drr.move_carm(rotations, translations)\n    source, rays = drr.detector.make_xrays(drr.rotations, drr.translations)\n    source_ = source.detach().cpu()\n    rays_ = rays.permute(2, 0, 1).detach().cpu()\n\n    ax = fig.add_subplot(2, 4, i+1, projection='3d')\n    ax.set(title=f\"$\\\\theta=\\\\frac{{{i}}}{{8}}\\pi$\")\n    ax.scatter(source_[0, 0, 0]  , source_[0, 0, 1]  , source_[0, 0, 2]  , c=\"black\")\n    ax.scatter(rays_[0].flatten(), rays_[1].flatten(), rays_[2].flatten(), c=torch.arange(25), cmap=\"jet\")\n\n    xs, ys, zs = rays_.reshape(3, -1)\n    for x, y, z in zip(xs, ys, zs):\n        ax.plot([source_[0, 0, 0], x], [source_[0, 0, 1], y], [source_[0, 0, 2], z], \"k\", alpha=0.2)\n\n    ax.set(xlabel=\"x\", ylabel=\"y\", zlabel=\"z\")\n    ax.set(xlim=[8,12], ylim=[-12,-8], zlim=[-42,-38])\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nfig = plt.figure()\nfor i in range(8):\n    theta = (0 / 8) * torch.pi\n    phi = (i / 8) * torch.pi\n    gamma = (0 / 8) * torch.pi\n    rotations = torch.Tensor([[theta, phi, gamma]])\n    \n    drr.move_carm(rotations, translations)\n    source, rays = drr.detector.make_xrays(drr.rotations, drr.translations)\n    source_ = source.detach().cpu()\n    rays_ = rays.permute(2, 0, 1).detach().cpu()\n\n    ax = fig.add_subplot(2, 4, i+1, projection='3d')\n    ax.set(title=f\"$\\\\phi=\\\\frac{{{i}}}{{8}}\\pi$\")\n    ax.scatter(source_[0, 0, 0]  , source_[0, 0, 1]  , source_[0, 0, 2]  , c=\"black\")\n    ax.scatter(rays_[0].flatten(), rays_[1].flatten(), rays_[2].flatten(), c=torch.arange(25), cmap=\"jet\")\n\n    xs, ys, zs = rays_.reshape(3, -1)\n    for x, y, z in zip(xs, ys, zs):\n        ax.plot([source_[0, 0, 0], x], [source_[0, 0, 1], y], [source_[0, 0, 2], z], \"k\", alpha=0.2)\n\n    ax.set(xlabel=\"x\", ylabel=\"y\", zlabel=\"z\")\n    ax.set(xlim=[8,12], ylim=[-12,-8], zlim=[-42,-38])\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nfig = plt.figure()\nfor i in range(8):\n    theta = (0 / 8) * torch.pi\n    phi = (0 / 8) * torch.pi\n    gamma = (i / 8) * torch.pi\n    rotations = torch.Tensor([[theta, phi, gamma]])\n    \n    drr.move_carm(rotations, translations)\n    source, rays = drr.detector.make_xrays(drr.rotations, drr.translations)\n    source_ = source.detach().cpu()\n    rays_ = rays.permute(2, 0, 1).detach().cpu()\n\n    ax = fig.add_subplot(2, 4, i+1, projection='3d')\n    ax.set(title=f\"$\\\\gamma=\\\\frac{{{i}}}{{8}}\\pi$\")\n    ax.scatter(source_[0, 0, 0]  , source_[0, 0, 1]  , source_[0, 0, 2]  , label=\"Source\", c=\"black\")\n    ax.scatter(rays_[0].flatten(), rays_[1].flatten(), rays_[2].flatten(), label=\"Targets\", c=torch.arange(25), cmap=\"jet\")\n\n    xs, ys, zs = rays_.reshape(3, -1)\n    for x, y, z in zip(xs, ys, zs):\n        ax.plot([source_[0, 0, 0], x], [source_[0, 0, 1], y], [source_[0, 0, 2], z], \"k\", alpha=0.2)\n\n    ax.set(xlabel=\"x\", ylabel=\"y\", zlabel=\"z\")\n    ax.set(xlim=[8,12], ylim=[-12,-8], zlim=[-42,-38])\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DiffDRR",
    "section": "",
    "text": "Auto-differentiable DRR synthesis and optimization in PyTorch\nDiffDRR is a PyTorch-based digitally reconstructed radiograph (DRR) generator that provides\nMost importantly, DiffDRR implements DRR synthesis as a PyTorch module, making it interoperable in deep learning pipelines."
  },
  {
    "objectID": "index.html#installation-guide",
    "href": "index.html#installation-guide",
    "title": "DiffDRR",
    "section": "Installation Guide",
    "text": "Installation Guide\nTo install DiffDRR from PyPI:\npip install diffdrr"
  },
  {
    "objectID": "index.html#usage",
    "href": "index.html#usage",
    "title": "DiffDRR",
    "section": "Usage",
    "text": "Usage\nThe following minimal example specifies the geometry of the projectional radiograph imaging system and traces rays through a CT volume:\n\nimport matplotlib.pyplot as plt\nimport torch\n\nfrom diffdrr.drr import DRR\nfrom diffdrr.data import load_example_ct\nfrom diffdrr.visualization import plot_drr\n\n# Read in the volume\nvolume, spacing = load_example_ct()\n\n# Initialize the DRR module for generating synthetic X-rays\ndrr = DRR(\n    volume,        # The CT volume as a numpy array\n    spacing,       # Voxel dimensions of the CT\n    sdr=300.0,     # Source-to-detector radius (half of the source-to-detector distance)\n    height=200,    # Height of the DRR (if width is not seperately provided, the generated image is square)\n    delx=4.0,      # Pixel spacing (in mm)\n    batch_size=1,  # How many batches of parameters will be passed = number of DRRs generated each forward pass\n).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Rotations and translations determine the viewing angle\n# They must have the same batch_size as was passed to the DRR constructor\n# Rotations are (yaw pitch roll)\n# Translations are (bx by bz)\nrotations = torch.tensor([[torch.pi, 0.0, torch.pi / 2]])\ntranslations = torch.tensor(volume.shape) * torch.tensor(spacing) / 2\ntranslations = translations.unsqueeze(0)\n\n# Generate the DRR\ndrr.move_carm(rotations, translations)\nimg = drr().detach()  # Only keep the graph if optimizing DRRs\nax = plot_drr(img)\nplt.show()\n\nOn a single NVIDIA RTX 2080 Ti GPU, producing such an image takes\n\n\n\n34.9 ms ± 90.5 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\nThe full example is available at tutorials/introduction.ipynb."
  },
  {
    "objectID": "index.html#application-6-dof-slice-to-volume-registration",
    "href": "index.html#application-6-dof-slice-to-volume-registration",
    "title": "DiffDRR",
    "section": "Application: 6-DoF Slice-to-Volume Registration",
    "text": "Application: 6-DoF Slice-to-Volume Registration\nWe demonstrate the utility of our auto-differentiable DRR generator by solving a 6-DoF registration problem with gradient-based optimization. Here, we generate two DRRs:\n\nA fixed DRR from a set of ground truth parameters\nA moving DRR from randomly initialized parameters\n\nTo solve the registration problem, we use gradient descent to maximize an image loss similarity metric between the two DRRs. This produces optimization runs like this:\n\nThe full example is available at tutorials/optimizers.ipynb."
  },
  {
    "objectID": "index.html#how-does-diffdrr-work",
    "href": "index.html#how-does-diffdrr-work",
    "title": "DiffDRR",
    "section": "How does DiffDRR work?",
    "text": "How does DiffDRR work?\nDiffDRR reformulates Siddon’s method (Siddon RL. Fast calculation of the exact radiological path for a three-dimensional CT array. Medical Physics, 2(12):252–5, 1985.), the canonical algorithm for calculating the radiologic path of an X-ray through a volume, as a series of vectorized tensor operations. This version of the algorithm is easily implemented in tensor algebra libraries like PyTorch to achieve a fast auto-differentiable DRR generator."
  },
  {
    "objectID": "index.html#citing-diffdrr",
    "href": "index.html#citing-diffdrr",
    "title": "DiffDRR",
    "section": "Citing DiffDRR",
    "text": "Citing DiffDRR\nIf you find DiffDRR useful in your work, please cite our paper (or the freely accessible arXiv version):\n@inproceedings{gopalakrishnanDiffDRR2022,\n    author    = {Gopalakrishnan, Vivek and Golland, Polina},\n    title     = {Fast Auto-Differentiable Digitally Reconstructed Radiographs for Solving Inverse Problems in Intraoperative Imaging},\n    year      = {2022},\n    booktitle = {Clinical Image-based Procedures: 11th International Workshop, CLIP 2022, Held in Conjunction with MICCAI 2022, Singapore, Proceedings},\n    series    = {Lecture Notes in Computer Science},\n    publisher = {Springer},\n    doi       = {https://doi.org/10.1007/978-3-031-23179-7_1},\n}"
  },
  {
    "objectID": "api/data.html",
    "href": "api/data.html",
    "title": "data",
    "section": "",
    "text": "source\n\nread_dicom\n\n read_dicom (dcmdir:Union[pathlib.Path,str], correct_zero:bool=True)\n\nRead a directory of DICOM files and return the volume and voxel spacings.\n\nsource\n\n\nload_example_ct\n\n load_example_ct ()\n\nLoad an example chest CT for demonstration purposes."
  },
  {
    "objectID": "api/detector.html",
    "href": "api/detector.html",
    "title": "Detector (C-Arm)",
    "section": "",
    "text": "Note, the Detector is usually initialized in the DRR module and shouldn’t need to be called directly.\n\nsource\n\nDetector\n\n Detector (sdr:float, height:int, width:int, delx:float, dely:float,\n           n_subsample:int|None=None, convention:str='diffdrr')\n\nConstruct a 6 DoF X-ray detector system. This model is based on a C-Arm.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsdr\nfloat\n\nSource-to-detector radius (half of the source-to-detector distance)\n\n\nheight\nint\n\nHeight of the X-ray detector\n\n\nwidth\nint\n\nWidth of the X-ray detector\n\n\ndelx\nfloat\n\nPixel spacing in the X-direction\n\n\ndely\nfloat\n\nPixel spacing in the Y-direction\n\n\nn_subsample\nint | None\nNone\nNumber of target points to randomly sample\n\n\nconvention\nstr\ndiffdrr\nEither diffdrr or deepdrr, order of basis matrix multiplication\n\n\n\n\nsource\n\n\nDetector.make_xrays\n\n Detector.make_xrays (rotations:torch.Tensor, translations:torch.Tensor)\n\nCreate source and target points for X-rays to trace through the volume.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nrotations\ntorch.Tensor\nVector of C-arm rotations (theta, phi, gamma) for azimuthal, polar, and roll angles\n\n\ntranslations\ntorch.Tensor\nVector of C-arm translations (bx, by, bz)"
  },
  {
    "objectID": "api/siddon.html",
    "href": "api/siddon.html",
    "title": "Siddon’s Method",
    "section": "",
    "text": "The process of generating a DRR models the geometry of an idealized projectional radiography system. Let \\(\\mathbf s \\in \\mathbb R^3\\) be the X-ray source, and \\(\\mathbf p \\in \\mathbb R^3\\) be a target pixel on the detector plane. Then \\(R(\\alpha) = \\mathbf s + \\alpha (\\mathbf p - \\mathbf s)\\) is a ray that originates from \\(\\mathbf s\\) (\\(\\alpha=0\\)), passes through the imaged volume, and hits the detector plane at \\(\\mathbf p\\) (\\(\\alpha=1\\)). The total energy attenuation experienced by the X-ray by the time it reaches pixel \\(\\mathbf p\\) is given by the following line integral:\n\\[\\begin{equation}\n    E(R) = \\|\\mathbf p - \\mathbf s\\|_2 \\int_0^1 \\mathbf V \\left( \\mathbf s + \\alpha (\\mathbf p - \\mathbf s) \\right) \\mathrm d\\alpha \\,,\n\\end{equation}\\]\nwhere \\(\\mathbf V : \\mathbb R^3 \\mapsto \\mathbb R\\) is the imaged volume. The term \\(\\|\\mathbf p - \\mathbf s\\|_2\\) endows the unit-free \\(\\mathrm d \\alpha\\) with the physical unit of length. For DRR synthesis, \\(\\mathbf V\\) is approximated by a discrete 3D CT volume, and Eq. (1) becomes\n\\[\\begin{equation}\n    E(R) = \\|\\mathbf p - \\mathbf s\\|_2 \\sum_{m=1}^{M-1} (\\alpha_{m+1} - \\alpha_m) \\mathbf V \\left[ \\mathbf s + \\frac{\\alpha_{m+1} + \\alpha_m}{2} (\\mathbf p - \\mathbf s) \\right] \\,,\n\\end{equation}\\]\nwhere \\(\\alpha_m\\) parameterizes the locations where ray \\(R\\) intersects one of the orthogonal planes comprising the CT volume, and \\(M\\) is the number of such intersections. Note that this model does not account for patterns of reflection and scattering that are present in real X-ray systems. While these simplifications preclude synthesis of realistic X-rays, the model in Eq. (2) has been widely and successfully used in slice-to-volume registration. Additionally, our approach of vectorizing DRR generation might also be interoperable with more sophisticated image synthesis models, an extension we examine further in the Discussion.\n\nsource\n\n\n\n siddon_raycast (source:torch.Tensor, target:torch.Tensor,\n                 volume:torch.Tensor, spacing:torch.Tensor,\n                 eps:float=1e-08)\n\nCompute Siddon’s method.\nSiddon’s method provides a parametric method to identify the plane intersections \\(\\{\\alpha_m\\}_{m=1}^M\\). Let \\(\\Delta X\\) be the CT voxel size in the \\(x\\)-direction and \\(b_x\\) be the location of the \\(0\\)-th plane in this direction. Then the intersection of ray \\(R\\) with the \\(i\\)-th plane in the \\(x\\)-direction is given by \\[\\begin{equation}\n    \\label{eqn:x-intersect}\n    \\alpha_x(i) = \\frac{b_x + i \\Delta X - \\mathbf s_x}{\\mathbf p_x - \\mathbf s_x} ,\n\\end{equation}\\] with analogous expressions for \\(\\alpha_y(\\cdot)\\) and \\(\\alpha_z(\\cdot)\\).\nWe can use Eq. (3) to compute the values \\(\\mathbf \\alpha_x\\) for all the intersections between \\(R\\) and the planes in the \\(x\\)-direction: \\[\\begin{equation*}\n    \\mathbf\\alpha_x = \\{ \\alpha_x(i_{\\min}), \\dots, \\alpha_x(i_{\\max}) \\} ,\n\\end{equation*}\\] where \\(i_{\\min}\\) and \\(i_{\\max}\\) denote the first and last intersections of \\(R\\) with the \\(x\\)-direction planes.\nDefining \\(\\mathbf\\alpha_y\\) and \\(\\mathbf\\alpha_z\\) analogously, we construct the array \\[\\begin{equation}\n    \\label{eqn:alphas}\n    \\mathbf\\alpha = \\mathrm{sort}(\\mathbf\\alpha_x, \\mathbf\\alpha_y, \\mathbf\\alpha_z) ,\n\\end{equation}\\] which contains \\(M\\) values of \\(\\alpha\\) parameterizing the intersections between \\(R\\) and the orthogonal \\(x\\)-, \\(y\\)-, and \\(z\\)-directional planes. We substitute values in the sorted set \\(\\mathbf\\alpha\\) into Eq. (2) to evaluate \\(E(R)\\), which corresponds to the intensity of pixel \\(\\mathbf p\\) in the synthesized DRR."
  },
  {
    "objectID": "api/siddon.html#drr-generation",
    "href": "api/siddon.html#drr-generation",
    "title": "Siddon’s Method",
    "section": "",
    "text": "The process of generating a DRR models the geometry of an idealized projectional radiography system. Let \\(\\mathbf s \\in \\mathbb R^3\\) be the X-ray source, and \\(\\mathbf p \\in \\mathbb R^3\\) be a target pixel on the detector plane. Then \\(R(\\alpha) = \\mathbf s + \\alpha (\\mathbf p - \\mathbf s)\\) is a ray that originates from \\(\\mathbf s\\) (\\(\\alpha=0\\)), passes through the imaged volume, and hits the detector plane at \\(\\mathbf p\\) (\\(\\alpha=1\\)). The total energy attenuation experienced by the X-ray by the time it reaches pixel \\(\\mathbf p\\) is given by the following line integral:\n\\[\\begin{equation}\n    E(R) = \\|\\mathbf p - \\mathbf s\\|_2 \\int_0^1 \\mathbf V \\left( \\mathbf s + \\alpha (\\mathbf p - \\mathbf s) \\right) \\mathrm d\\alpha \\,,\n\\end{equation}\\]\nwhere \\(\\mathbf V : \\mathbb R^3 \\mapsto \\mathbb R\\) is the imaged volume. The term \\(\\|\\mathbf p - \\mathbf s\\|_2\\) endows the unit-free \\(\\mathrm d \\alpha\\) with the physical unit of length. For DRR synthesis, \\(\\mathbf V\\) is approximated by a discrete 3D CT volume, and Eq. (1) becomes\n\\[\\begin{equation}\n    E(R) = \\|\\mathbf p - \\mathbf s\\|_2 \\sum_{m=1}^{M-1} (\\alpha_{m+1} - \\alpha_m) \\mathbf V \\left[ \\mathbf s + \\frac{\\alpha_{m+1} + \\alpha_m}{2} (\\mathbf p - \\mathbf s) \\right] \\,,\n\\end{equation}\\]\nwhere \\(\\alpha_m\\) parameterizes the locations where ray \\(R\\) intersects one of the orthogonal planes comprising the CT volume, and \\(M\\) is the number of such intersections. Note that this model does not account for patterns of reflection and scattering that are present in real X-ray systems. While these simplifications preclude synthesis of realistic X-rays, the model in Eq. (2) has been widely and successfully used in slice-to-volume registration. Additionally, our approach of vectorizing DRR generation might also be interoperable with more sophisticated image synthesis models, an extension we examine further in the Discussion.\n\nsource\n\n\n\n siddon_raycast (source:torch.Tensor, target:torch.Tensor,\n                 volume:torch.Tensor, spacing:torch.Tensor,\n                 eps:float=1e-08)\n\nCompute Siddon’s method.\nSiddon’s method provides a parametric method to identify the plane intersections \\(\\{\\alpha_m\\}_{m=1}^M\\). Let \\(\\Delta X\\) be the CT voxel size in the \\(x\\)-direction and \\(b_x\\) be the location of the \\(0\\)-th plane in this direction. Then the intersection of ray \\(R\\) with the \\(i\\)-th plane in the \\(x\\)-direction is given by \\[\\begin{equation}\n    \\label{eqn:x-intersect}\n    \\alpha_x(i) = \\frac{b_x + i \\Delta X - \\mathbf s_x}{\\mathbf p_x - \\mathbf s_x} ,\n\\end{equation}\\] with analogous expressions for \\(\\alpha_y(\\cdot)\\) and \\(\\alpha_z(\\cdot)\\).\nWe can use Eq. (3) to compute the values \\(\\mathbf \\alpha_x\\) for all the intersections between \\(R\\) and the planes in the \\(x\\)-direction: \\[\\begin{equation*}\n    \\mathbf\\alpha_x = \\{ \\alpha_x(i_{\\min}), \\dots, \\alpha_x(i_{\\max}) \\} ,\n\\end{equation*}\\] where \\(i_{\\min}\\) and \\(i_{\\max}\\) denote the first and last intersections of \\(R\\) with the \\(x\\)-direction planes.\nDefining \\(\\mathbf\\alpha_y\\) and \\(\\mathbf\\alpha_z\\) analogously, we construct the array \\[\\begin{equation}\n    \\label{eqn:alphas}\n    \\mathbf\\alpha = \\mathrm{sort}(\\mathbf\\alpha_x, \\mathbf\\alpha_y, \\mathbf\\alpha_z) ,\n\\end{equation}\\] which contains \\(M\\) values of \\(\\alpha\\) parameterizing the intersections between \\(R\\) and the orthogonal \\(x\\)-, \\(y\\)-, and \\(z\\)-directional planes. We substitute values in the sorted set \\(\\mathbf\\alpha\\) into Eq. (2) to evaluate \\(E(R)\\), which corresponds to the intensity of pixel \\(\\mathbf p\\) in the synthesized DRR."
  },
  {
    "objectID": "api/visualization.html",
    "href": "api/visualization.html",
    "title": "visualization",
    "section": "",
    "text": "source\n\nplot_drr\n\n plot_drr (img:torch.Tensor, title:Optional[str]=None,\n           ticks:Optional[bool]=True,\n           axs:Optional[matplotlib.axes._axes.Axes]=None)\n\nPlot an image generated by a DRR module.\n\nsource\n\n\nanimate\n\n animate (out:Union[str,pathlib.Path], df:pandas.core.frame.DataFrame,\n          drr:diffdrr.drr.DRR, ground_truth:Optional[torch.Tensor]=None,\n          verbose:bool=True, **kwargs)\n\nAnimate the optimization of a DRR.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nout\nstr | pathlib.Path\n\nSavepath\n\n\ndf\npandas.DataFrame\n\n\n\n\ndrr\nDRR\n\n\n\n\nground_truth\ntorch.Tensor | None\nNone\n\n\n\nverbose\nbool\nTrue\n\n\n\nkwargs\n\n\n\n\n\n\ndf is a pandas.DataFrame with columns [\"theta\", \"phi\", \"gamma\", \"bx\", \"by\", \"bz\"]. Each row in df is an iteration of optimization with the updated values for that timestep."
  },
  {
    "objectID": "api/drr.html",
    "href": "api/drr.html",
    "title": "DRR",
    "section": "",
    "text": "X-ray pose parameters\nThe viewing angle for the DRR (known generally in computer graphics as pose parameters) is parameterized by the following:\n\nsdr : Source-to-Detector radius (half of the source-to-detector distance)\ntheta : Azimuthal angle\nphi : Polar angle\ngamma : Plane rotation angle\nbx : X-dir translation\nby : Y-dir translation\nbz : Z-dir translation\n\nTranslational (bx, by, bz) and rotational (theta, phi, gamma) parameters are grouped. The rotational pose parameters are detailed in Spherical Coordiantes Tutorial.\n\nsource\n\n\nDRR\n\n DRR (volume:np.ndarray, spacing:np.ndarray, sdr:float, height:int,\n      delx:float, width:int|None=None, dely:float|None=None,\n      p_subsample:float|None=None, reshape:bool=True,\n      convention:str='diffdrr', batch_size:int=1)\n\nTorch module that computes differentiable digitally reconstructed radiographs.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nvolume\nnp.ndarray\n\nCT volume\n\n\nspacing\nnp.ndarray\n\nDimensions of voxels in the CT volume\n\n\nsdr\nfloat\n\nSource-to-detector radius for the C-arm (half of the source-to-detector distance)\n\n\nheight\nint\n\nHeight of the rendered DRR\n\n\ndelx\nfloat\n\nX-axis pixel size\n\n\nwidth\nint | None\nNone\nWidth of the rendered DRR (if not provided, set to height)\n\n\ndely\nfloat | None\nNone\nY-axis pixel size (if not provided, set to delx)\n\n\np_subsample\nfloat | None\nNone\nProportion of pixels to randomly subsample\n\n\nreshape\nbool\nTrue\nReturn DRR with shape (b, h, w)\n\n\nconvention\nstr\ndiffdrr\nEither diffdrr or deepdrr, order of basis matrix multiplication\n\n\nbatch_size\nint\n1\nNumber of DRRs to generate per forward pass\n\n\n\nThe forward pass of the DRR module is used to generate DRRs from the volume. The pose parameters (i.e., viewing angles) from which these imges are generated as stored as nn.Parameters of the module. To update these parameters, pass a new set of rotations and translations to the DRR.move_carm function. This allows the pose parameters to be optimized with any PyTorch optimizer.\n\nsource\n\n\nDRR.move_carm\n\n DRR.move_carm (rotations:torch.Tensor, translations:torch.Tensor)\n\n\nsource\n\n\nDRR.forward\n\n DRR.forward ()\n\nGenerate DRR with rotations and translations parameters."
  },
  {
    "objectID": "api/utils.html",
    "href": "api/utils.html",
    "title": "utils",
    "section": "",
    "text": "source\n\nreshape_subsampled_drr\n\n reshape_subsampled_drr (img:torch.Tensor,\n                         detector:diffdrr.detector.Detector,\n                         batch_size:int)"
  },
  {
    "objectID": "api/metrics.html",
    "href": "api/metrics.html",
    "title": "metrics",
    "section": "",
    "text": "source\n\nXCorr2\n\n XCorr2 (zero_mean_normalized=False)\n\nCompute the normalized cross-correlation between two images with the same shape."
  }
]